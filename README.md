
# ğŸ¥ Predicting 30-Day Readmission Risk in Diabetic Patients Using Machine Learning

![Python](https://img.shields.io/badge/Language-Python-blue.svg)
![Model](https://img.shields.io/badge/Model-LightGBM-success)
![License](https://img.shields.io/badge/License-MIT-lightgrey)

This project aims to predict **30-day hospital readmissions** for diabetic patients using structured electronic health records (EHR) from the UCI Diabetes dataset. Through advanced data preprocessing, domain-informed feature engineering, class imbalance handling, and machine learning, we developed a robust model with strong recall and explainability.

## ğŸ“‚ Project Structure

```
Readmission-Prediction-Project/
â”œâ”€â”€ README.md
â”œâ”€â”€ Documentation.pdf                # Full project report 
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ diabetic_data.csv
â”‚   â””â”€â”€ IDS_mapping.csv
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ Readmission.ipynb            # End-to-end notebook with code & markdowns
â”œâ”€â”€ images/                          # SHAP and feature importance plots
â””â”€â”€ LICENSE
```

## ğŸ¯ Objective

To build an interpretable machine learning model that identifies diabetic patients at risk of hospital readmission within 30 days of discharge.

## ğŸ“Š Dataset Overview

- **Source**: UCI Diabetes 130-US hospitals dataset
- **Records**: ~100,000 hospital encounters
- **Features**: Demographics, diagnoses, procedures, medications, and more
- **Target Variable**: `readmitted` (<30 days vs. not)

## ğŸ§ª Workflow Summary

1. **Data Cleaning & Preparation**
   - Handled missing values, removed irrelevant features
   - Standardized categorical codes and merged mappings
2. **Feature Engineering**
   - Domain-aware features: insulin use, age-hospitalization interaction, discharge flags
   - Created risk indicators and binary medication features
3. **EDA & Insights**
   - Explored trends in readmissions, diagnosis categories, and medication patterns
4. **Model Development**
   - Algorithms: Logistic Regression, Random Forest, LightGBM, XGBoost
   - Used pipelines, cross-validation, and hyperparameter tuning
   - Applied SMOTE & class weights to handle imbalance
5. **Evaluation**
   - Precision, recall, F1-score, confusion matrix
   - Feature importance & SHAP plots for explainability

## ğŸ§  Best Model Summary

| Model                  | Recall | Precision | F1-Score | Accuracy |
|------------------------|--------|-----------|----------|----------|
| LightGBM (Tuned)       | 0.62   | 0.18      | 0.28     | 0.65     |
| XGBoost (Tuned)        | 0.60   | 0.18      | 0.27     | 0.65     |
| Logistic Regression    | 0.54   | 0.17      | 0.25     | 0.66     |
| SMOTE + LightGBM (0.1) | 0.81   | 0.16      | 0.24     | 0.52     |

## ğŸ” Key Risk Factors Identified

- **number_inpatient** â€“ strongest indicator of future readmission
- **discharged_to_facility**, **discharged_home** â€“ key discharge destinations
- **insulin_used**, **diabetesMedication** â€“ treatment signals
- **age_inpatient_interaction** â€“ combined demographic + utilization risk
- **num_lab_procedures**, **num_medications** â€“ signal patient complexity

## ğŸš¨ Real-World Considerations

Although our tuned LightGBM model achieved a **recall of 0.62** and **F1-score of 0.28**, it's important to understand that structured EHR data alone has limitations in predicting real-world outcomes.

Several important factors influencing readmission risk are **not captured in this dataset**, such as:

- Mental health status or behavioral factors
- Medication adherence post-discharge
- Availability of caregiver support at home
- Environmental or socioeconomic stressors
- Sudden medical complications or accidents

These limitations explain why precision remained moderate (~18%) despite strong recall. In healthcare, **missing a high-risk patient** can be more costly than flagging a false positive â€” which is why recall was prioritized in this project.

This model is therefore best used as a **clinical decision support tool** â€” helping care teams identify and prioritize at-risk patients, while still relying on human expertise for final decisions.



## ğŸ“¦ Installation

```bash
pip install -r requirements.txt
```

## âš™ï¸ Reproducibility

- Python 3.9+
- Trained and evaluated using Jupyter Notebook
- All preprocessing handled via `Pipeline()`

## ğŸ§  Author

**Shruthin Reddy**  
Master's Student â€“ University of Louisville  
Email: [shruthinreddysainapuram@gmail.com]  

## ğŸ“Œ License

This project is licensed under the MIT License.
